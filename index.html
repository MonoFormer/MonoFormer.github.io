<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
  	<title>MonoFormer</title>
      <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
          if you update and want to force Facebook to re-scrape. -->
  	<meta property="og:image" content="./resources/teasor.png"/>
  	<meta property="og:title" content="MonoFormer: One Transformer for Both Diffusion and Autoregression" />
  	<meta property="og:description" content="VidProM is the first dataset featuring 1.67 million unique text-to-video prompts and 6.69 million videos generated from 4 different state-of-the-art diffusion models. It inspires many exciting new research areas, such as Text-to-Video Prompt Engineering, Efficient Video Generation, Fake Video Detection, and Video Copy Detection for Diffusion Models." />
   

    <!-- Add your Google Analytics tag here -->
    <script async
            src="https://www.googletagmanager.com/gtag/js?id=UA-97476543-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>

</head>

<body>
<div class="container">
    <div class="title">
        MonoFormer: One Transformer for Both Diffusion and Autoregression
    </div>

    <div class="venue">
        Arxiv 2024
    </div>

    <br><br>
    <div class="author">
        Chuyang Zhao<sup>1</sup>
    </div>
    <div class="author">
        Yuxing Song<sup>1</sup>
    </div>
    <div class="author">
        <a href="https://wangwenhao0716.github.io">Wenhao Wang</a><sup>2</sup>
    </div>
    <div class="author">
        <a href="https://scholar.google.com/citations?user=pnuQ5UsAAAAJ">Haocheng Feng</a><sup>1</sup>
    </div>
    <div class="author">
        <a href="https://scholar.google.com/citations?user=1wzEtxcAAAAJ">Errui Ding</a><sup>1</sup>
    </div>
    <div class="author">
        <a href="https://yifansun-reid.github.io/">Yifan Sun</a><sup>1</sup>
    </div>
    <div class="author">
        <a href="https://scholar.google.com/citations?user=lWRYa3wAAAAJ">Xinyan Xiao</a><sup>1</sup>
    </div>
    <div class="author">
        <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a><sup>1</sup>
    </div>
    <br><br>

    <div class="affiliation"><sup>1&nbsp;</sup>Baidu VIS</div>
    <div class="affiliation"><sup>2&nbsp;</sup>University of Technology Sydney</div>

    <br><br>

    <div class="links"><a href="https://monoformer.github.io/">[Paper]</a></div>
    <div class="links"><a href="https://monoformer.github.io/">[Huggingface]</a></div>
    <div class="links"><a href="https://monoformer.github.io/">[Github]</a></div>

    <br><br>
    

    <br><br>

    <img style="width: 80%;" src="./resources/teasor.png" alt="Teaser figure."/>
    <br>
    <p style="width: 80%;">
        MonoFormer aims to build and train one transformer for both autoregression and diffusion. Examples of MonoFormer for both image generation and text generation tasks. Left: Class-conditional image generation. Middle: Text-to-image generation. Right: Text-to-text generation.

    <br><br>
    <hr>

    <h1>Abstract</h1>
    <p style="width: 80%;">
        Most existing multimodality methods use separate backbones for autoregression-based discrete text generation and diffusion-based continuous visual generation,
or the same backbone by discretizing the visual data to use autoregression for both
text and visual generation. In this paper, we propose to study a simple idea: share
one transformer for both autoregression and diffusion. The feasibility comes from
two main aspects: (i) Transformer is successfully applied to diffusion for visual
generation, and (ii) transformer training for autoregression and diffusion is very
similar, and the difference merely lies in that diffusion uses bidirectional attention
mask and autoregression uses causal attention mask. Experimental results show
that our approach achieves comparable image generation performance to current
state-of-the-art methods as well as maintains the text generation capability.
    </p>

    <br><br>
    <hr>

    <h1>Architecture</h1>
    <img style="width: 80%;" src="./resources/otv3.png"
         alt="Architecture"/>

    <br>
    <p style="width: 80%;">
        Our approach MonoFormer trains the autoregressive transformer and the diffusion transformer, which share the weights, and uses causal attention mask and bidirectional attention mask,
respectively. During training, the input of the transformer for autoregression is the text token embeddings, and the output is embeddings that are further processed for text generation. The input for
diffusion is the noised latent embeddings, and the output is embeddings that are used to predict the noise.
    <br><br>  
    <hr>

    <h1>Quantative Experiments</h1>
    <img style="width: 80%;" src="./resources/exp_1.png"
         alt="Quantative Experiments"/>
    <br>
    <p style="width: 80%; text-align: center;">
    Performance on ImageNet 256Ã—256 benchmark.
    <br><br>
    </p>

    <hr>

    <img style="width: 80%;" src="./resources/exp_2.png"
         alt="Quantative Experiments"/>
    <br>
    <p style="width: 80%; text-align: center;">
        Performance on commonsense reasoning tasks.
    <br><br>  
    </p>
    <hr>

   <img style="width: 80%;" src="./resources/exp_3.png"
         alt="Quantative Experiments"/>
    <br>
    <p style="width: 80%;">
        (a) The effect of transformer initialization for image generation, measured using the FiD-
10K metric on ImageNet. (b) The effect of transformer initialization for text generation, measured
by the average commonsense reasoning score. (c) The effect of bidirectional attention mask for
image generation.
    <br><br>  
    <hr>

   

   

    
    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://arxiv.org/abs/2403.06098">
            <img class="layered-paper-big" width="100%" src="./resources/VidProM.jpg" alt="Paper thumbnail"/>
        </a>
    </div>
    <div class="paper-info">
        <h3>VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models</h3>
        <p>Wenhao Wang and Yi Yang</p>
        <p>Arxiv, 2024.</p>
        <pre><code>@article{wang2024vidprom,
  title={VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models},
  author={Wang, Wenhao and Yang, Yi},
  journal={arXiv preprint arXiv:2403.06098},
  year={2024}
}</code></pre>
    </div>

    <br><br>
    <hr>

   <h1>Contact</h1>
    <p style="width: 80%;">
        If you have any questions, feel free to contact Chuyang Zhao (zhaochuyang@baidu.com).
    </p>

      <br><br>
    <hr>

 
    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful project</a>, and inherits the modifications made by <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a> and <a href="https://github.com/elliottwu/webpage-template">Shangzhe Wu</a>.
        The code can be found <a href="https://github.com/monoformer/monoformer.github.io">here</a>.
    </p>

    <br><br>
</div>

</body>

</html>
